{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SetupDeterministicTransitionByStateSet2Agent(object):\n",
    "    def __init__(self, stateSet, actionSet, goalStates = []):\n",
    "        self.stateSet = stateSet\n",
    "        # create a joint state set from a single agent state set, add terminal state to the set\n",
    "        self.jointStateSet = [(s1, s2) for s1, s2 in itertools.product(stateSet, stateSet) if s1 != s2] + ['terminal']\n",
    "        self.jointActionSet = list(itertools.product(actionSet, actionSet))\n",
    "        self.goalStates = goalStates\n",
    "\n",
    "    def __call__(self):\n",
    "        transitionTable = {state: self.getStateTransition(state) for state in self.jointStateSet}\n",
    "        return (transitionTable)\n",
    "\n",
    "    def getStateTransition(self, state):\n",
    "        actionTransitionDistribution = {action: self.getStateActionTransition(state, action) for action in\n",
    "                                        self.jointActionSet}\n",
    "        return (actionTransitionDistribution)\n",
    "\n",
    "    def getStateActionTransition(self, currentState, action):\n",
    "        reachedGoalState = any([currentPos in self.goalStates for currentPos in currentState])\n",
    "        if currentState == 'terminal' or reachedGoalState:\n",
    "            transitionDistribution = {'terminal': 1.0}\n",
    "        else:\n",
    "            transitionDistribution = self.getTransitionDistribution(currentState, action)\n",
    "        return (transitionDistribution)\n",
    "\n",
    "    def getTransitionDistribution(self, state, action):\n",
    "        # if you directly apply the action to the current state, what the potential next state for each agent is\n",
    "        potentialNextState = tuple([self.addTuples(agentS, agentA) for agentS, agentA in zip(state, action)])\n",
    "        agent1NextState = potentialNextState[0]\n",
    "        agent2NextState = potentialNextState[1]\n",
    "\n",
    "        agent1Fixed = False\n",
    "        agent2Fixed = False\n",
    "        # if a move takes you off the board, you cannot take it and instead that agent remains stationary, if fixed = true, that agent must remain stationary\n",
    "        if agent1NextState not in self.stateSet:\n",
    "            agent1NextState = state[0]\n",
    "            agent1Fixed = True\n",
    "        if agent2NextState not in self.stateSet:\n",
    "            agent2NextState = state[1]\n",
    "            agent2Fixed = True\n",
    "\n",
    "        # resulting joint state from taking into account moves off the board - is it a viable move\n",
    "        onBoardPotentialNextState = (agent1NextState, agent2NextState)\n",
    "\n",
    "        # if it is viable, agents will not collide and it should be in the joint state set\n",
    "        if onBoardPotentialNextState in self.jointStateSet:\n",
    "            return ({onBoardPotentialNextState: 1.0})\n",
    "\n",
    "        # if it is not in the joint state set, there is a collision\n",
    "        if agent1NextState == agent2NextState:\n",
    "            # collision 1: one agent runs into the stationary other\n",
    "            if action[0] == (0, 0) or action[1] == (0, 0):\n",
    "                return ({state: 1.0})\n",
    "            # collision 2: one agent tries to move off the board (and must stay stationary), the other collides into it there\n",
    "            elif agent1Fixed or agent2Fixed:\n",
    "                return ({state: 1.0})\n",
    "            # collision 3: a collision on the board, probabilistically sample who moves and who stays\n",
    "            else:\n",
    "                agent1Moves = (agent1NextState, state[1])\n",
    "                agent2Moves = (state[0], agent2NextState)\n",
    "                return ({agent1Moves: .5, agent2Moves: .5})\n",
    "\n",
    "    def addTuples(self, tuple1, tuple2):\n",
    "        lengthOfShorterTuple = min(len(tuple1), len(tuple2))\n",
    "        summedTuple = tuple([tuple1[i] + tuple2[i] for i in range(lengthOfShorterTuple)])\n",
    "        return (summedTuple)\n",
    "\n",
    "    \n",
    "class SetupRewardTable2AgentDistanceCost(object):\n",
    "    def __init__(self, transitionTable, goalStates = [], trapStates = []):\n",
    "        self.transitionTable = transitionTable\n",
    "        self.goalStates = goalStates\n",
    "        self.trapStates = trapStates\n",
    "        \n",
    "    def __call__(self, goalReward = 10, trapCost = -100, costOfNoMovement = .1):\n",
    "        rewardTable = {state:{action: {nextState: self.applyRewardFunction(state, action, nextState, goalReward, trapCost, costOfNoMovement) \\\n",
    "                                        for nextState in nextStateDict.keys()} \n",
    "                                for action, nextStateDict in actionDict.items()} \n",
    "                        for state, actionDict in self.transitionTable.items()}\n",
    "        return(rewardTable)\n",
    "\n",
    "    def applyRewardFunction(self, state, action, nextState, goalReward, trapCost, costOfNoMovement):\n",
    "        # terminal state has no reward or cost\n",
    "        if state == 'terminal':\n",
    "            return(0)\n",
    "        #Unless already in the terminal state, incur the cost of action\n",
    "        movementCosts = self.getCosts(state, action, costOfNoMovement)\n",
    "        \n",
    "        # if the intended next state is a special tile, \n",
    "        # the cost/reward of s, a, s' corresponds to the value of that tile\n",
    "        specialTileCosts = self.getSpecialTileRewards(nextState, trapCost, goalReward)\n",
    "        return(movementCosts + specialTileCosts)\n",
    "\n",
    "    def getSpecialTileRewards(self, state, trapCost, goalReward):\n",
    "        # if the next state is a special tile, the agent receives the rewards/costs of that location\n",
    "        agent1State = state[0]\n",
    "        agent2State = state[1]\n",
    "        #get special rewards -- if either state is the goal state or the trap state\n",
    "        reward = 0\n",
    "\n",
    "        if (agent1State in self.goalStates) or (agent2State in self.goalStates):\n",
    "            reward = reward + abs(goalReward)\n",
    "        if agent1State in self.trapStates or agent2State in self.trapStates:\n",
    "            reward = reward - abs(trapCost)\n",
    "        return(reward)\n",
    "\n",
    "    def getCosts(self, state, action, costOfNoMovement):\n",
    "        # move costs - if in the goal state, no move cost \n",
    "        # because transition will move agent to terminal state no matter what\n",
    "        agent1State = state[0]\n",
    "        agent2State = state[1]\n",
    "        if agent1State in self.goalStates or agent2State in self.goalStates:\n",
    "            moveCost = 0\n",
    "        else:\n",
    "            moveCost = sum([self.getCostOfDistance(agentAction, costOfNoMovement) \n",
    "                for agentAction in action])\n",
    "        return(moveCost)\n",
    "\n",
    "    def getCostOfDistance(self, action, costOfNoMovement, nullAction = (0,0)):\n",
    "        #Need to fix this for two agents\n",
    "        if action == nullAction:\n",
    "            return(-abs(costOfNoMovement))\n",
    "        else:\n",
    "            actionDistance = sum([abs(actionCoordinate) for actionCoordinate in action])\n",
    "            return(-actionDistance)\n",
    "\n",
    "import math\n",
    "\n",
    "class BoltzmannValueIteration(object):\n",
    "    def __init__(self, transitionTable, rewardTable, valueTable, convergenceTolerance, discountingFactor, beta):\n",
    "        self.transitionTable = transitionTable\n",
    "        self.rewardTable  = rewardTable\n",
    "        self.valueTable = valueTable\n",
    "        self.convergenceTolerance = convergenceTolerance\n",
    "        self.gamma = discountingFactor\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(self):\n",
    "        \n",
    "        delta = self.convergenceTolerance*100\n",
    "        while(delta > self.convergenceTolerance):\n",
    "            delta = 0\n",
    "            for state, actionDict in self.transitionTable.items():\n",
    "                valueOfStateAtTimeT = self.valueTable[state]\n",
    "                qforAllActions = [self.getQValue(state, action) for action in actionDict.keys()]\n",
    "                self.valueTable[state] = max(qforAllActions) \n",
    "                delta = max(delta, abs(valueOfStateAtTimeT-self.valueTable[state]))\n",
    "        policyTable = {state:self.getBoltzmannPolicy(state) for state in self.transitionTable.keys()}\n",
    "\n",
    "        return([self.valueTable, policyTable])\n",
    "    \n",
    "    def getBoltzmannPolicy(self, state, printStatments = False):\n",
    "        exponents = [self.beta*self.getQValue(state, action) for action in self.transitionTable[state].keys()]\n",
    "        actions = [action for action in self.transitionTable[state].keys()]\n",
    "\n",
    "        # Scale to [0,700] if there are exponents larger than 700\n",
    "        if len([exponent for exponent in exponents if exponent>700])>0:\n",
    "            if printStatments:\n",
    "                print(\"scaling exponents to [0,700]... On State:\")\n",
    "                print(state)\n",
    "            exponents = [700*(exponent/max(exponents)) for exponent in exponents]\n",
    "\n",
    "        statePolicy = {action: math.exp(exponent) for exponent, action in zip(exponents,actions)}\n",
    "        normalizedPolicy = self.normalizeDictionaryValues(statePolicy)\n",
    "        return(normalizedPolicy)\n",
    "\n",
    "    def getQValue(self, state, action):\n",
    "        nextStatesQ = [prob*(self.rewardTable[state][action][nextState] \\\n",
    "                             + self.gamma*self.valueTable[nextState]) \\\n",
    "                      for nextState, prob in self.transitionTable[state][action].items()]\n",
    "\n",
    "        qValue = sum(nextStatesQ)\n",
    "        return(qValue)\n",
    "    \n",
    "    def normalizeDictionaryValues(self, unnormalizedDictionary):\n",
    "        totalSum = sum(unnormalizedDictionary.values())\n",
    "        normalizedDictionary = {originalKey: val/totalSum for originalKey, val in unnormalizedDictionary.items()}\n",
    "        return(normalizedDictionary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridNumberX = 5\n",
    "gridNumberY = 5\n",
    "states = list(itertools.product(range(gridNumberX), range(gridNumberY)))\n",
    "actions = [(-1, 0), (0, 1), (1, 0), (0, -1), (0, 0)]\n",
    "\n",
    "# unambiguous goals\n",
    "# goals = [(4, 2), (0, 3)]\n",
    "# goals = [(4, 0), (2, 3)]\n",
    "\n",
    "# ambiguous goals\n",
    "goals = [(1, 4), (2, 1)]\n",
    "gettransition = SetupDeterministicTransitionByStateSet2Agent(states, actions, goals)\n",
    "transitionTable = gettransition()\n",
    "\n",
    "getReward = SetupRewardTable2AgentDistanceCost(transitionTable, goals)\n",
    "rewardTable = getReward()\n",
    "\n",
    "convergence = .000001\n",
    "gamma = .95\n",
    "valueTable = {state: 0 for state in transitionTable.keys()}\n",
    "beta = 3\n",
    "\n",
    "performValueIteration = BoltzmannValueIteration(transitionTable, rewardTable, valueTable, convergence, gamma, beta)\n",
    "optimalValues, policy = performValueIteration()\n",
    "\n",
    "initialState = ((0, 0), (4, 4))\n",
    "\n",
    "states = list(itertools.product(range(gridNumberX), range(gridNumberY)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionSpace = [(-1, 0), (1, 0), (0, 1), (0, -1), (0,0)]\n",
    "policy_initState = policy[initialState]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, 0): 0.021482853568032607,\n",
       " (1, 0): 0.318687597543442,\n",
       " (0, 1): 0.318687597543442,\n",
       " (0, -1): 0.021482853568032607,\n",
       " (0, 0): 0.31965909777705115}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{action1: sum([policy_initState[(action1, action2)] for action2 in actionSpace]) for action1 in actionSpace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, 0): 0.318687597543442,\n",
       " (1, 0): 0.03810529223482553,\n",
       " (0, 1): 0.03810529223482553,\n",
       " (0, -1): 0.03810529223482553,\n",
       " (0, 0): 0.5669965257520818}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{action2: sum([policy_initState[(action1, action2)] for action1 in actionSpace]) for action2 in actionSpace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, 0): 0.016879879554834595,\n",
       " (1, 0): 0.016879879554834595,\n",
       " (0, 1): 0.016879879554834595,\n",
       " (0, -1): 0.016879879554834595,\n",
       " (0, 0): 0.2511680793241036}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{action2: policy_initState[((1, 0), action2)] for action2 in actionSpace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, 0): 0.016879879554834595,\n",
       " (1, 0): 0.0002574408880416658,\n",
       " (0, 1): 0.0002574408880416658,\n",
       " (0, -1): 0.0002574408880416658,\n",
       " (0, 0): 0.0038306513490730144}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{action2: policy_initState[((0, -1), action2)] for action2 in actionSpace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(-1, 0): 0.2511680793241036,\n",
       " (1, 0): 0.0038306513490730144,\n",
       " (0, 1): 0.0038306513490730144,\n",
       " (0, -1): 0.0038306513490730144,\n",
       " (0, 0): 0.056999064405728515}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{action2: policy_initState[((0, 0), action2)] for action2 in actionSpace}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
